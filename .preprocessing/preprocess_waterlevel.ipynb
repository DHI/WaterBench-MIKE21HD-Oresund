{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Website for downloading data: https://data.marine.copernicus.eu/product/INSITU_BAL_PHYBGCWAV_DISCRETE_MYNRT_013_032/download?dataset=cmems_obs-ins_bal_phybgcwav_mynrt_na_irr_202311--ext--history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = pd.date_range(start=\"2014-01-01\", end=\"2023-12-31\", freq='30min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dfs(df):\n",
    "    # Since it is timezone UTC, we can safely remove the 'Z' and 'T' characters and convert to datetime\n",
    "    df[\"datetime\"]=pd.to_datetime(df['time'].str.replace('T', ' ').str.replace('Z', ''))\n",
    "    # Only choose every 30 minute data. It is different how frequent the data is, hence the funny looking indexing\n",
    "    df_30min = df[df['datetime'].isin(times)]\n",
    "    # Select only data with valueQc < 2 (good or probably good data)\n",
    "    df_good = df_30min[df_30min['valueQc'] < 2]\n",
    "    # Select only the columns we want to keep\n",
    "    df_new = df_good.set_index('datetime')[['platformId', 'value']]\n",
    "    # rename 'value' to 'SLEV' \n",
    "    df_new.rename(columns={'value': 'water_level'}, inplace=True)\n",
    "    # Round values\n",
    "    df_new['water_level'] = df_new['water_level'].round(4)\n",
    "    # Now split df_new into separate dataframes for each platformId and collect them in a lookup dictionary\n",
    "    platforms = df_new['platformId'].unique()\n",
    "    dfs = {}\n",
    "    for platform in platforms:\n",
    "        dfs[platform] = df_new[df_new['platformId'] == platform]\n",
    "\n",
    "    return dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"cmems_obs_2014_2015.csv\", \"cmems_obs_2016_2017.csv\", \"cmems_obs_2018_2019.csv\", \"cmems_obs_2020_2021.csv\", \"cmems_obs_2022_2023.csv\"]\n",
    "\n",
    "df =pd.read_csv(f\"raw_data/{files[0]}\", index_col=0, parse_dates=True)\n",
    "dfs_all = prepare_dfs(df)\n",
    "\n",
    "# Loop over files (from old to new) and add to dfs_all\n",
    "for f in files[1:]:\n",
    "    df =pd.read_csv(f\"raw_data/{f}\", index_col=0, parse_dates=True)\n",
    "    dfs_next = prepare_dfs(df)\n",
    "\n",
    "    # Concatenate the dataframes in dfs0 and dfs1 according to their keys, avoiding duplicated rows\n",
    "    for key in dfs_all.keys():\n",
    "        if key in dfs_next.keys():\n",
    "            dfs_next[key] = dfs_next[key].loc[dfs_next[key].index > dfs_all[key].index[-1]]\n",
    "            # Concatenate new data to existing data\n",
    "            dfs_all[key] = pd.concat([dfs_all[key], dfs_next[key]])\n",
    "\n",
    "    # If new stations are present, add them to the dictionary\n",
    "    for key in dfs_next.keys():\n",
    "        if key not in dfs_all.keys():\n",
    "            dfs_all[key] = dfs_next[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Drogden', 'Klagshamn', 'Koege', 'Dragor', 'Skanor', 'Hornbaek', 'Kobenhavn', 'Viken', 'Vedbaek', 'Barseback', 'NordreRose', 'Flinten7', 'MalmoHamn', 'Helsingborg'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dfs_all for duplicate indices\n",
    "for key in dfs_all.keys():\n",
    "    if dfs_all[key].index.duplicated().any():\n",
    "        print(f\"Duplicate indices in {key}\")\n",
    "\n",
    "dfs_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through the keys and save the dataframes to csv\n",
    "# Add to the name the first present year in data and the last present year\n",
    "skip_station = [\"Viken\"] # Skip this station since it is too far out of the domain\n",
    "for key in dfs_all.keys():\n",
    "    if key in skip_station:\n",
    "        continue\n",
    "    dfs_all[key].drop(columns=[\"platformId\"]).to_csv(f\"../observations/{key}_wl.csv\", date_format='%Y-%m-%dT%H:%M:%SZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FMROM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
